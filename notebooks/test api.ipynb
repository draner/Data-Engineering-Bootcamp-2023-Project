{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POC : Extract data from 4chan API\n",
    "\n",
    "This notebook is a proof of concept to extract data from the 4chan API and store it as Parquet files.\n",
    "\n",
    "We need to extract 2 types of files:\n",
    "\n",
    "- 1 file for threads named `threads_{timestamp}.parquet`\n",
    "- 1 file per thread named `posts_{thread_id}_{number_of_posts}.parquet`\n",
    "\n",
    "Once we have those files, we can use the scripts developed here to build our data pipelines.\n",
    "\n",
    "\n",
    "[Documentation of the library used to get new data](https://basc-py4chan.readthedocs.io/en/latest/index.html)\n",
    "\n",
    "## Step 1 : Get the list of all threads on /pol/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import basc_py4chan\n",
    "\n",
    "# First, we need to create a board object. This is the object that will be used to access the board.\n",
    "board = basc_py4chan.Board('pol')\n",
    "\n",
    "# Now we can retrieve all the threads on the board.\n",
    "threads = board.get_all_threads(expand=False)\n",
    "threads_ids = board.get_all_thread_ids()\n",
    "print('There are', len(threads), 'active threads on /pol/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every thread, we can populate a dataframe with the thread's information.\n",
    "threads_df = pd.DataFrame()\n",
    "for i, thread in enumerate(threads):\n",
    "    thread_dict = {'thread_id': threads_ids[i],\n",
    "                   'is_sticky': thread.sticky,\n",
    "                   'is_closed': thread.closed,\n",
    "                   'topic': thread.topic.text_comment,\n",
    "                   'number_of_posts': len(thread.all_posts),\n",
    "                   'url': thread.url}\n",
    "    new_row = pd.DataFrame(thread_dict, index=[0])\n",
    "    threads_df = pd.concat([threads_df, new_row], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the exported parquet file named 'threads_{timestamp}.parquet'\n",
    "timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')\n",
    "threads_df.to_parquet(f'data/threads_{timestamp}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the parquet file and print the first 5 rows\n",
    "threads_df_parquet = pd.read_parquet('threads_20230324_130829.parquet')\n",
    "threads_df_parquet.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Success](https://media.giphy.com/media/a0h7sAqON67nO/giphy.gif)\n",
    "\n",
    "## Step 2 : Get the list of all posts for each thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and export a parquet file with every post for every thread\n",
    "\n",
    "for i, thread in enumerate(threads):\n",
    "    thread_df = pd.DataFrame()\n",
    "    for post in thread.all_posts:\n",
    "        post_dict = {'thread_id': threads_ids[i],\n",
    "                     'post_id': post.post_id,\n",
    "                     'poster_id': post.poster_id,\n",
    "                     'poster_name': post.name,\n",
    "                     'is_op': post.is_op,\n",
    "                     'tripcode': post.tripcode,\n",
    "                     'email': post.email,\n",
    "                     'subject': post.subject,\n",
    "                     'comment': post.text_comment,\n",
    "                     'has_file': post.has_file,\n",
    "                     'post_datetime': post.datetime,\n",
    "                     'url': post.url}\n",
    "        if post_dict['has_file']:\n",
    "            post_dict['file_name'] = post.file.filename_original\n",
    "            post_dict['file_extension'] = post.file.file_extension\n",
    "        else:\n",
    "            post_dict['file_name'] = None\n",
    "            post_dict['file_extension'] = None\n",
    "\n",
    "        new_row = pd.DataFrame(post_dict, index=[0])\n",
    "        thread_df = pd.concat([thread_df, new_row], axis=0)\n",
    "    thread_df.to_parquet(f'data/posts_{threads_ids[i]}_{thread_df.shape[0]}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a parquet file and print the first 5 rows\n",
    "\n",
    "thread_df_parquet = pd.read_parquet('data\\posts_420846947.parquet')\n",
    "thread_df_parquet.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Success](https://media.giphy.com/media/Od0QRnzwRBYmDU3eEO/giphy.gif)\n",
    "\n",
    "We now have proof that we can extract data from the 4chan API and store it as Parquet files.\n",
    "We can now use this approach to build our data pipelines.\n",
    "\n",
    "## Test the output from the first pipeline, API to GCP Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = pd.read_parquet('threads_20230324_13.parquet')\n",
    "posts = pd.read_parquet('posts_20230324_13.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for dbt API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# request dbt api to get the list of jobs\n",
    "\n",
    "account_id = '148403'\n",
    "auth_token = \"65a52d89ef6730fff272b2794035fed23d569fd0\"\n",
    "\n",
    "url = 'https://cloud.getdbt.com/api/v2/accounts/{accountId}/jobs/'\n",
    "\n",
    "# Add token to headers and define content-type\n",
    "\n",
    "headers = { 'Token': auth_token, 'Content-Type': 'application/json' }\n",
    "\n",
    "# Make the request and print the response\n",
    "\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "print(response.status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
